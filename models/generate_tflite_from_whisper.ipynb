{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Install Tranformers and datasets"
      ],
      "metadata": {
        "id": "c5g9NTF_Ixad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "w4VPaSlnHUvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate TF Saved momdel"
      ],
      "metadata": {
        "id": "W9XP25uhJl44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import WhisperProcessor, WhisperFeatureExtractor, TFWhisperForConditionalGeneration, WhisperTokenizer\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny.en\")\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny.en\", predict_timestamps=True)\n",
        "processor = WhisperProcessor(feature_extractor, tokenizer)\n",
        "model = TFWhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
        "# Loading dataset\n",
        "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "\n",
        "inputs = feature_extractor(\n",
        "    ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"tf\"\n",
        ")\n",
        "input_features = inputs.input_features\n",
        "\n",
        "# Generating Transcription\n",
        "generated_ids = model.generate(input_features=input_features)\n",
        "print(generated_ids)\n",
        "transcription = processor.tokenizer.decode(generated_ids[0])\n",
        "print(transcription)\n",
        "model.save('/content/tf_whisper_saved')"
      ],
      "metadata": {
        "id": "vpYwMmgyHf0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert saved model to TFLite model"
      ],
      "metadata": {
        "id": "TY_79jFEJYyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "saved_model_dir = '/content/tf_whisper_saved'\n",
        "tflite_model_path = 'whisper.tflite'\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open(tflite_model_path, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "owez2zvzHl-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create generation-enabled TF Lite model\n",
        "\n",
        "The solution consists in defining a model whose serving function is the generation call. Here's an example of how to do it:"
      ],
      "metadata": {
        "id": "a8VJQuHJKzl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenerateModel(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    super(GenerateModel, self).__init__()\n",
        "    self.model = model\n",
        "\n",
        "  @tf.function(\n",
        "    # shouldn't need static batch size, but throws exception without it (needs to be fixed)\n",
        "    input_signature=[\n",
        "      tf.TensorSpec((1, 80, 3000), tf.float32, name=\"input_features\"),\n",
        "    ],\n",
        "  )\n",
        "  def serving(self, input_features):\n",
        "    outputs = self.model.generate(\n",
        "      input_features,\n",
        "      max_new_tokens=450, #change as needed\n",
        "      return_dict_in_generate=True,\n",
        "    )\n",
        "    return {\"sequences\": outputs[\"sequences\"]}\n",
        "\n",
        "saved_model_dir = '/content/tf_whisper_saved'\n",
        "tflite_model_path = 'whisper-tiny.en.tflite'\n",
        "\n",
        "generate_model = GenerateModel(model=model)\n",
        "tf.saved_model.save(generate_model, saved_model_dir, signatures={\"serving_default\": generate_model.serving})\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open(tflite_model_path, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "wSMrJo7hJ95c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loaded model... now with generate!\n",
        "tflite_model_path = 'whisper-tiny.en.tflite'\n",
        "interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "\n",
        "tflite_generate = interpreter.get_signature_runner()\n",
        "generated_ids = tflite_generate(input_features=input_features)[\"sequences\"]\n",
        "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "transcription"
      ],
      "metadata": {
        "id": "dLkMa_36PgW-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}